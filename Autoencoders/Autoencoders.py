# -*- coding: utf-8 -*-
"""ACML 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1epn-Xgb6CH46W4R2Gqevi2u12Heg8LNJ
Authors: Dimitrios Gagatsis // Christos Kaparakis
# Import the Libraries
"""

from keras.models import Sequential
from keras.layers import BatchNormalization, Dense, Conv2D, UpSampling2D, MaxPooling2D, Dropout, Flatten, InputLayer, LeakyReLU

from keras.optimizers import Adam
from keras.callbacks import Callback

from keras.utils import np_utils # To transform labels in categorical
from keras.datasets import cifar10 # Load the dataset

import matplotlib.pyplot as plt
import numpy as np

from skimage.color import rgb2gray, rgb2ycbcr, ycbcr2rgb
from skimage.io import imread, imshow

from sklearn.model_selection import train_test_split

from keras import backend as K

from google.colab import files

"""## Data Loading"""

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

print('x_train:', x_train.shape)
print('x_test:', x_test.shape)
print('y_train:', y_train.shape)
print('y_test:', y_test.shape)

plt.imshow(x_test[0])

"""# Data Normalization"""

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

y_train = y_train.astype('float32')
y_test = y_test.astype('float32')

x_train = x_train/255
x_test = x_test/255

print(x_train.shape)
print(x_test.shape)

"""## Split the data in 80% training, 10% validation and 10% testing"""

data_x = np.concatenate((x_train, x_test))
data_y = np.concatenate((y_train, y_test))
# Divide our dataset into training (80%), validation (10%) and test (10%)
x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=42, shuffle= True)
x_valid, x_test, y_valid, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=42, shuffle= True)

"""## Neural Network

# Create the model
"""

model = Sequential()
model.add(InputLayer(input_shape=(32,32,3)))
#1
model.add(Conv2D(8, (3,3), strides=1, padding='same', activation='relu'))
#2
model.add(MaxPooling2D(pool_size=(2,2)))
#3
model.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#4
model.add(MaxPooling2D(pool_size=(2,2)))
#5
model.add(Conv2D(16,(3,3), strides=1, padding='same', activation='relu'))
#6
model.add(UpSampling2D(size=(2, 2)))
#7
model.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#8
model.add(UpSampling2D(size=(2, 2)))
#9
model.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))

#compile the model
model.compile(optimizer='adam',loss='mse')
model.summary()

modelf = model.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

print('Size of the latent space is:', 1024)

x = np.arange(1,21,1)
plt.plot(x, modelf.history['loss'],label='Loss')
plt.plot(x, modelf.history['val_loss'],label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('MSE')
plt.xticks(x)
plt.legend()
plt.show()

# Evaluate 
scores = model.evaluate(x_test, x_test)
scores

# The size of the latent space representation of the above network is: 1024

"""# Experiments with different architectures
#Removing intermediate layers
"""

model1 = Sequential()  
model1.add(InputLayer(input_shape=(32,32,3)))
#1
model1.add(MaxPooling2D(pool_size=(2,2)))
#2
model1.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#3
model1.add(MaxPooling2D(pool_size=(2,2)))
#4
model1.add(Conv2D(16,(3,3), strides=1, padding='same', activation='relu'))
#5
model1.add(UpSampling2D(size=(2, 2)))
#6
model1.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#7
model1.add(UpSampling2D(size=(2, 2)))
#8
model1.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))

model1.compile(optimizer='adam',loss='mse')
model1.summary()
model1f = model1.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

print('Size of the latent space is:', 4096)

model1.history
scores1 = model1.evaluate(x_test, x_test)
scores1

model2 = Sequential()
model2.add(InputLayer(input_shape=(32,32,3)))
#1
model2.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#2
model2.add(MaxPooling2D(pool_size=(2,2)))
#3
model2.add(Conv2D(16,(3,3), strides=1, padding='same', activation='relu'))
#4
model2.add(UpSampling2D(size=(2, 2)))
#5
model2.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#6
model2.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))

model2.compile(optimizer='adam',loss='mse')
model2.summary()
model2f = model2.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

scores2 = model2.evaluate(x_test, x_test)
scores2

print('Size of the latent space is:', 4096)

"""# Trying different number of channels"""

# Chaning the number of channels
model3 = Sequential()
model3.add(InputLayer(input_shape=(32,32,3)))
#1
model3.add(Conv2D(32, (3,3), strides=1, padding='same', activation='relu'))
#2
model3.add(MaxPooling2D(pool_size=(2,2)))
#3
model3.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu'))
#4
model3.add(MaxPooling2D(pool_size=(2,2)))
#5
model3.add(Conv2D(128,(3,3), strides=1, padding='same', activation='relu'))
#6
model3.add(UpSampling2D(size=(2, 2)))
#7
model3.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu'))
#8
model3.add(UpSampling2D(size=(2, 2)))
#9
model3.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))

model3.compile(optimizer='adam',loss='mse')
model3.summary()
model3f = model3.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

scores3 = model3.evaluate(x_test, x_test)
scores3

print('Size of the latent space is:', 8192)

# Out model trained Total params: 168,771 and the evaluate score is 0.0010793713154271245.
# To conclude, when the model has bigger filter sizes like the last one, the number of parameters
# is it training are increasing and the evaluation score drops at a good level.

"""# Changing the kernel sizes (filters)"""

# Changing the kerner sizes to (5,5)
model4 = Sequential()
model4.add(InputLayer(input_shape=(32,32,3)))
#1
model4.add(Conv2D(8, (5,5), strides=1, padding='same', activation='relu'))
#2
model4.add(MaxPooling2D(pool_size=(2,2)))
#3
model4.add(Conv2D(12,(5,5), strides=1, padding='same', activation='relu'))
#4
model4.add(MaxPooling2D(pool_size=(2,2)))
#5
model4.add(Conv2D(16,(5,5), strides=1, padding='same', activation='relu'))
#6
model4.add(UpSampling2D(size=(2, 2)))
#7
model4.add(Conv2D(12,(5,5), strides=1, padding='same', activation='relu'))
#8
model4.add(UpSampling2D(size=(2, 2)))
#9
model4.add(Conv2D(3, (5,5), strides=1, padding='same', activation='relu'))

model4.compile(optimizer='adam',loss='mse')
model4.summary()
model4f = model4.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

scores4 = model4.evaluate(x_train, x_train)
scores4

print('Size of the latent space is:', 576)

# Changing the kerner sizes to (7,7)
model5 = Sequential()
model5.add(InputLayer(input_shape=(32,32,3)))
#1
model5.add(Conv2D(8, (7,7), strides=1, padding='same', activation='relu'))
#2
model5.add(MaxPooling2D(pool_size=(2,2)))
#3
model5.add(Conv2D(12,(7,7), strides=1, padding='same', activation='relu'))
#4
model5.add(MaxPooling2D(pool_size=(2,2)))
#5
model5.add(Conv2D(16,(7,7), strides=1, padding='same', activation='relu'))
#6
model5.add(UpSampling2D(size=(2, 2)))
#7
model5.add(Conv2D(12,(7,7), strides=1, padding='same', activation='relu'))
#8
model5.add(UpSampling2D(size=(2, 2)))
#9
model5.add(Conv2D(3, (7,7), strides=1, padding='same', activation='relu'))

model5.compile(optimizer='adam',loss='mse')
model5.summary()
model5f = model5.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

scores5 = model5.evaluate(x_test, x_test)
scores5

print('Size of the latent space is:', 256)

# Almost the same with the (5,5)

"""# Changing the strides configurations"""

# Chaning the strides to 3
model6 = Sequential()
model6.add(InputLayer(input_shape=(32,32,3)))
#1
model6.add(Conv2D(8, (3,3), strides=3, padding='same', activation='relu'))
#2
model6.add(MaxPooling2D(pool_size=(2,2)))
#3
model6.add(Conv2D(12,(3,3), strides=3, padding='same', activation='relu'))
#4
model6.add(MaxPooling2D(pool_size=(2,2)))
#5
model6.add(Conv2D(16,(3,3), strides=3, padding='same', activation='relu'))
#6
model6.add(UpSampling2D(size=(2, 2)))
#7
model6.add(Conv2D(12,(3,3), strides=3, padding='same', activation='relu'))
#8
model6.add(UpSampling2D(size=(2, 2)))
#9
model6.add(Conv2D(3, (3,3), strides=3, padding='same', activation='relu'))

model6.compile(optimizer='adam',loss='mse')
model6.summary()
model6f = model6.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

scores6 = model6.evaluate(x_test, x_test)
scores6

print('Size of the latent space is:', 16)

"""# Changing padding configuration"""

# Changing the padding configuration into 'valid' for all the layers except the first and the last.
model7 = Sequential()
model7.add(InputLayer(input_shape=(32,32,3)))
#1
model7.add(Conv2D(8, (3,3), strides=1, padding='same', activation='relu'))
#2
model7.add(MaxPooling2D(pool_size=(2,2)))
#3
model7.add(Conv2D(12,(3,3), strides=1, padding='valid', activation='relu'))
#4
model7.add(MaxPooling2D(pool_size=(2,2)))
#5
model7.add(Conv2D(16,(3,3), strides=1, padding='valid', activation='relu'))
#6
model7.add(UpSampling2D(size=(2, 2)))
#7
model7.add(Conv2D(12,(3,3), strides=1, padding='valid', activation='relu'))
#8
model7.add(UpSampling2D(size=(2, 2)))
#9
model7.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))
#10
model7.add(UpSampling2D(size=(2,2)))

model7.compile(optimizer='adam',loss='mse')
model7.summary()
model7f = model7.fit(x_train, x_train, epochs=20, validation_data=(x_valid,x_valid),shuffle=True)

scores7 = model7.evaluate(x_test, x_test)
scores7

"""# Colorization

## Creating the grayscale Images
"""

img_rows = x_train.shape[1]
img_cols = x_train.shape[2]
channels = x_train.shape[3]


x_train_gray = rgb2gray(x_train)
x_valid_gray = rgb2gray(x_valid)
x_test_gray = rgb2gray(x_test)

x_train_chrom = rgb2ycbcr(x_train)
y, cb, cr = x_train_chrom[:,:,:,0], x_train_chrom[:,:,:,1], x_train_chrom[:,:,:,2]
chrom_train= np.zeros(shape=(x_train_chrom.shape[0],x_train_chrom.shape[1],x_train_chrom.shape[2],2))
chrom_train[:,:,:,0] = cb
chrom_train[:,:,:,1] = cr

x_valid_chrom = rgb2ycbcr(x_valid)
y, cb, cr = x_valid_chrom[:,:,:,0], x_valid_chrom[:,:,:,1], x_valid_chrom[:,:,:,2]
chrom_valid= np.zeros(shape=(x_valid_chrom.shape[0],x_valid_chrom.shape[1],x_valid_chrom.shape[2],2))
chrom_valid[:,:,:,0] = cb
chrom_valid[:,:,:,1] = cr

x_test_chrom = rgb2ycbcr(x_test)
y, cb, cr = x_test_chrom[:,:,:,0], x_test_chrom[:,:,:,1], x_test_chrom[:,:,:,2]
chrom_test= np.zeros(shape=(x_test_chrom.shape[0],x_test_chrom.shape[1],x_test_chrom.shape[2],2))
chrom_test[:,:,:,0] = cb
chrom_test[:,:,:,1] = cr

# reshape images to row x col x channel for CNN input
x_train_gray = x_train_gray.reshape(x_train_gray.shape[0], img_rows, img_cols, 1)
x_test_gray = x_test_gray.reshape(x_test_gray.shape[0], img_rows, img_cols, 1)
x_valid_gray = x_valid_gray.reshape(x_valid_gray.shape[0], img_rows, img_cols, 1)


# reshape images to row x col x channel for CNN output/validation
x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, channels)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, channels)
x_valid = x_valid.reshape(x_valid.shape[0], img_rows, img_cols, channels)

#import cv2
from google.colab.patches import cv2_imshow
imshow(x_test_gray[1].reshape(32,32))

# Choosing the best model after the exmperiments we made
# The best model is the model with 2 layers less 
# and with bigger bumber of channels

model_final1 = Sequential()

model_final1.add(InputLayer(input_shape=(32,32,1)))
#1
model_final1.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#2
model_final1.add(MaxPooling2D(pool_size=(2,2)))
#3
model_final1.add(Conv2D(16,(3,3), strides=1, padding='same', activation='relu'))
#4
model_final1.add(UpSampling2D(size=(2, 2)))
#5
model_final1.add(Conv2D(12,(3,3), strides=1, padding='same', activation='relu'))
#6
model_final1.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))

# Run the final model
model_final1.compile(optimizer='Adam',loss='mse')
model_final1.summary()
model_final1_fit = model_final1.fit(x_train_gray, x_train, epochs=100, validation_data=(x_valid_gray,x_valid),shuffle=True)

scores = model_final1.evaluate(x_test_gray, x_test)
scores

# predict the autoencoder output from test data
x_decoded = model_final1.predict(x_test_gray)

# #combine chrominance with greyscale image to reconstruct RGB image
# chrom_decoded = np.empty(shape=(x_decoded.shape[0],x_test_chrom.shape[1],x_test_chrom.shape[2],3))
# chrom_decoded[:,:,:,0] = x_test_gray[:,:,:,0]*255
# chrom_decoded[:,:,:,1] = x_decoded[:,:,:,0]
# chrom_decoded[:,:,:,2] = x_decoded[:,:,:,1]
# decoded_rgb = ycbcr2rgb(chrom_decoded)

plt.imshow(x_decoded[15])

# display the 1st 50 colorized images vs the 1st 50 reconstructed images
imgs = x_test[:50]
imgs = imgs.reshape((5, 10, img_rows, img_cols, channels))
imgs1 = x_decoded[:50]
imgs1 = imgs1.reshape((5, 10, img_rows, img_cols, channels))
fimgs = []
for i in range(5):
  fimgs.append(imgs[i])
  fimgs.append(imgs1[i])
imgs = np.hstack([np.vstack(i) for i in fimgs])
plt.figure()
plt.axis('off')
plt.title('True colors - Reconstruction')
plt.imshow(imgs, interpolation='none')
plt.show()

"""# Combination of different architecture / hyperparameters / optimization"""

model_final2 = Sequential()
model_final2.add(InputLayer(input_shape=(32,32,1)))
#
model_final2.add(Conv2D(8,(3,3), strides=1, padding='same', activation='relu'))
#
model_final2.add(Conv2D(16,(3,3), strides=1, padding='same', activation='relu'))
#
model_final2.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu'))
#
model_final2.add(Conv2D(128,(3,3), strides=1, padding='same', activation='relu'))
#
model_final2.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu'))
#
model_final2.add(Conv2D(32,(3,3), strides=1, padding='same', activation='relu'))
#
model_final2.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))

adamOptimizer = Adam(lr=0.0001)
# Run the final model
model_final2.compile(optimizer='Adam',loss='mse')
model_final2.summary()
model_final2_fit = model_final2.fit(x_train_gray, x_train, epochs=100, validation_data=(x_valid_gray,x_valid),shuffle=True)

scores = model_final2.evaluate(x_test_gray, x_test)
scores

# predict the autoencoder output from test data
x_decoded = model_final2.predict(x_test_gray)

# #combine chrominance with greyscale image to reconstruct RGB image
# chrom_decoded = np.empty(shape=(x_decoded.shape[0],x_test_chrom.shape[1],x_test_chrom.shape[2],3))
# chrom_decoded[:,:,:,0] = x_test_gray[:,:,:,0]*255
# chrom_decoded[:,:,:,1] = x_decoded[:,:,:,0]
# chrom_decoded[:,:,:,2] = x_decoded[:,:,:,1]
# decoded_rgb = ycbcr2rgb(chrom_decoded)

plt.imshow(x_decoded[27])

# display the 1st 50 colorized images vs the 1st 50 reconstructed images
imgs = x_test[:50]
imgs = imgs.reshape((5, 10, img_rows, img_cols, channels))
imgs1 = x_decoded[:50]
imgs1 = imgs1.reshape((5, 10, img_rows, img_cols, channels))
fimgs = []
for i in range(5):
  fimgs.append(imgs[i])
  fimgs.append(imgs1[i])
imgs = np.hstack([np.vstack(i) for i in fimgs])
plt.figure()
plt.axis('off')
plt.title('True colors - Reconstruction')
plt.imshow(imgs, interpolation='none')
plt.show()

# Model final 3

model_final3 = Sequential()
model_final3.add(InputLayer(input_shape=(32,32,1)))
#1
model_final3.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu'))
#2
model_final3.add(MaxPooling2D(pool_size=(2,2)))
#3
model_final3.add(Conv2D(128,(3,3), strides=1, padding='same', activation='relu'))
#4
model_final3.add(Conv2D(256,(3,3), strides=1, padding='same', activation='relu'))
#5
model_final3.add(Conv2D(128,(3,3), strides=1, padding='same', activation='relu'))
#6
model_final3.add(Conv2D(64,(3,3), strides=1, padding='same', activation='relu'))
#7
model_final3.add(UpSampling2D(size=(2, 2)))
#8
model_final3.add(Conv2D(32,(3,3), strides=1, padding='same', activation='relu'))
#9
model_final3.add(Conv2D(3, (3,3), strides=1, padding='same', activation='relu'))

# Run the final model
model_final3.compile(optimizer=Adam(lr=0.001),loss='mse')
model_final3.summary()
model_final3_fit = model_final3.fit(x_train_gray, x_train, epochs=20, validation_data=(x_valid_gray,x_valid),shuffle=True)

